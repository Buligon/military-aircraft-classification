{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN9rTbn6gH16"
      },
      "source": [
        "# Download e extração do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap3PPT07x5jB"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPtYTIYFx6HU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31zH-lHHx8D2"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg5r-X4ryCe0"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d a2015003713/militaryaircraftdetectiondataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYE4idtgyDuN"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('militaryaircraftdetectiondataset.zip', 'r')\n",
        "zip_ref.extractall('/dataset')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DRSZM3ugpEV"
      },
      "source": [
        "# Imports necessários"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWQUg4pNxswH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, applications, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3qpN__gvDZ"
      },
      "source": [
        "#Função para plotar histórico do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POYrGyFtgfvJ"
      },
      "outputs": [],
      "source": [
        "def plota_historico_modelo(historico_modelo):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    axs[0].plot(range(1,len(historico_modelo.history['accuracy'])+1),\n",
        "                historico_modelo.history['accuracy'],'r')\n",
        "    axs[0].plot(range(1,len(historico_modelo.history['val_accuracy'])+1),\n",
        "                historico_modelo.history['val_accuracy'],'b')\n",
        "    axs[0].set_title('Acurácia do Modelo')\n",
        "    axs[0].set_ylabel('Acuracia')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1, len(historico_modelo.history['accuracy']) + 1, 1))\n",
        "\n",
        "    axs[0].legend(['training accuracy', 'validation accuracy'], loc='best')\n",
        "\n",
        "    axs[1].plot(range(1,len(historico_modelo.history['loss'])+1),\n",
        "                historico_modelo.history['loss'],'r')\n",
        "    axs[1].plot(range(1,len(historico_modelo.history['val_loss'])+1),\n",
        "                historico_modelo.history['val_loss'],'b')\n",
        "    axs[1].set_title('Perda/Loss do Modelo')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1, len(historico_modelo.history['loss']) + 1, 1))\n",
        "    axs[1].legend(['training loss', 'validation Loss'], loc='best')\n",
        "    fig.savefig('historico_modelo_mod01.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J6sBVbdg6nU"
      },
      "source": [
        "# Carregando dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxb3gfr5xswK"
      },
      "outputs": [],
      "source": [
        "dataset_directory = '/dataset/crop'  # se rodar localmente, utilizar 'dataset/crop'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfk4VkA8xswL",
        "outputId": "e62f8504-167c-4e6f-ed83-ac777bedb960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 19270 files belonging to 43 classes.\n",
            "Using 13489 files for training.\n",
            "Found 19270 files belonging to 43 classes.\n",
            "Using 5781 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 40\n",
        "image_size = (128, 128)\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_directory,\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size = batch_size,\n",
        "    seed=123\n",
        ")\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    dataset_directory,\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size = batch_size,\n",
        "    seed=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNcy2C3ljtoM"
      },
      "outputs": [],
      "source": [
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "test_data = validation_dataset.take(val_batches//5)\n",
        "val_data = validation_dataset.skip(val_batches//5)\n",
        "\n",
        "class_names = train_dataset.class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsB9qECy9foT"
      },
      "source": [
        "The prefetch method is used to improve data loading performance by asynchronously prefetching data samples from the input dataset.\n",
        "The buffer_size parameter, set to AUTOTUNE, lets TensorFlow determine an appropriate buffer size for this prefetch operation based on\n",
        "available system resources and other factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfc-UkIRjv0k"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_prefetch = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "val_prefetch = val_data.prefetch(buffer_size=AUTOTUNE)\n",
        "test_prefetch = test_data.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_prefetch = train_prefetch.map(lambda x, y: (x, tf.one_hot(y, len(class_names))))\n",
        "val_prefetch = val_data.map(lambda x, y: (x, tf.one_hot(y, len(class_names))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3nkNkWkxswM"
      },
      "source": [
        "# Treinando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjfS5Xyj4Hu"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.experimental.preprocessing.RandomRotation(factor=0.4, fill_mode=\"wrap\"),\n",
        "    layers.experimental.preprocessing.RandomContrast(factor=0.2)\n",
        "])\n",
        "\n",
        "preprocess_input = applications.resnet50.preprocess_input\n",
        "\n",
        "pretrained_model = ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(128, 128, 3),\n",
        "    weights='imagenet'\n",
        ")\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "global_avg = layers.GlobalAveragePooling2D()\n",
        "output_layer = layers.Dense(len(class_names), activation='softmax')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFbpCs4CxswN",
        "outputId": "8003cc61-e559-4950-8d24-022e02fa3c1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " tf.__operators__.getitem_4  (None, 128, 128, 3)       0         \n",
            "  (SlicingOpLambda)                                              \n",
            "                                                                 \n",
            " tf.nn.bias_add_4 (TFOpLamb  (None, 128, 128, 3)       0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d_4  (None, 2048)              0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 43)                88107     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23675819 (90.32 MB)\n",
            "Trainable params: 88107 (344.17 KB)\n",
            "Non-trainable params: 23587712 (89.98 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.keras.Input(shape=(128, 128, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(inputs)\n",
        "x = pretrained_model(x)\n",
        "x = global_avg(x)\n",
        "outputs = output_layer(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwFt5XFxswN"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=Adam(optimizers.schedules.CosineDecay(0.0001, 500)),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPLZ5fRLxswO"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_prefetch, validation_data=(val_prefetch), epochs = 5)\n",
        "\n",
        "plota_historico_modelo(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBiszjkp0gYK"
      },
      "outputs": [],
      "source": [
        "total_layers = len(pretrained_model.layers)\n",
        "print(f\"Número de layers do ResNet50: {total_layers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDbMPO75xswO"
      },
      "outputs": [],
      "source": [
        "pretrained_model.trainable = True\n",
        "for layer in pretrained_model.layers[:110]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI-VWJGzxswP"
      },
      "outputs": [],
      "source": [
        "optimizer = optimizers.RMSprop(learning_rate = optimizers.schedules.CosineDecay(0.000001, 500))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Js7zNKRMxswP"
      },
      "outputs": [],
      "source": [
        "arquivo_modelo = \"modelo.h5\"\n",
        "checkpointer = ModelCheckpoint(arquivo_modelo,\n",
        "                               monitor='val_loss',\n",
        "                               verbose=1,\n",
        "                               save_best_only=True)\n",
        "\n",
        "history_final = model.fit(train_prefetch,\n",
        "                          validation_data=(val_prefetch),\n",
        "                          epochs = 10,\n",
        "                          callbacks=[checkpointer],\n",
        "                          initial_epoch=history.epoch[-1])\n",
        "plota_historico_modelo(history_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqBq3VHRAK9J"
      },
      "source": [
        "# Testando modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDJ3aKK8pV3c"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = test_prefetch.as_numpy_iterator().next()\n",
        "predicted_labels = np.argmax(model.predict(image_batch), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKsrieVWpb5x"
      },
      "outputs": [],
      "source": [
        "label_vs_prediction = np.transpose(np.vstack((label_batch, predicted_labels)))\n",
        "print(label_vs_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCpEpyZEpsaa"
      },
      "outputs": [],
      "source": [
        "for image, true_label, predicted_label in zip(image_batch, label_batch, predicted_labels):\n",
        "    if true_label != predicted_label:\n",
        "        true_class_name = class_names[true_label]\n",
        "        predicted_class_name = class_names[predicted_label]\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(image.astype(\"uint8\"))\n",
        "        plt.title(f'True Label: {true_class_name}, Predicted Label: {predicted_class_name}')\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('modelo.h5')\n",
        "\n",
        "def get_true_and_predicted_labels(model, dataset):\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        true_labels.extend(tf.argmax(labels, axis=1))\n",
        "        predicted_labels.extend(np.argmax(model.predict(images), axis=1))\n",
        "\n",
        "    return true_labels, predicted_labels\n",
        "\n",
        "true_labels, predicted_labels = get_true_and_predicted_labels(model, val_prefetch)\n",
        "\n",
        "confusion = confusion_matrix(true_labels, predicted_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(confusion, class_names):\n",
        "    plt.imshow(confusion, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    \n",
        "    plt.xticks(tick_marks, class_names, rotation=45, ha='right') \n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    fmt = 'd'\n",
        "    thresh = confusion.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
        "        plt.text(j, i, format(confusion[i, j], fmt),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if confusion[i, j] > thresh else \"black\")\n",
        "    \n",
        "    plt.ylabel('Correct Classification')\n",
        "    plt.xlabel('Prediction')\n",
        "\n",
        "plt.figure(figsize=(12, 10)) \n",
        "\n",
        "plot_confusion_matrix(confusion, class_names)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
